# Mirrors
HF_ENDPOINT=https://hf-mirror.com

# LLMs
# Learn more: https://docs.mem0.ai/components/llms/config#master-list-of-all-params-in-config
## LLMs Common Params
# Reminder: When using Deepseek models in Volcengine, LLM_PROVIDER still "deepseek", but model name should be Endpoint ID like: ep-20250209205841-25wzv .
LLM_PROVIDER=openai
LLM_MODEL=gpt-4o
TEMPERATURE=0.2
MAX_TOKENS=1500
TOP_P=0.1
TOP_K=10

## OpenAI Extra
OPENAI_API_KEY=
# if you need to use a different API endpoint (defaults to “https://api.openai.com/v1”).
OPENAI_API_BASE=

## DeepSeek Extra
# deepseek-chat: The model has been fully upgraded to DeepSeek-V3, with the interface remaining unchanged. You can call DeepSeek-V3 by specifying model='deepseek-chat'.
# deepseek-reasoner: R1 model currently does not support json output and function call, see https://api-docs.deepseek.com/zh-cn/guides/reasoning_model 
DEEPSEEK_API_KEY=
# if you need to use a different API endpoint (defaults to “https://api.deepseek.com”).
DEEPSEEK_API_BASE=


# DBs
# Supported Vector Databases detail see that: https://docs.mem0.ai/components/vectordbs/dbs/chroma
VECTOR_STORE_PROVIDER=qdrant
# The name of the collection to store the vectors, default is mem0
VECTOR_STORE_COLLECTION_NAME=mem0

# Fill the db host and port that mem0-api can reach.
# when db and mem0-api both running in same docker network, use the docker container name: mem0-qdrant
# When mem0-api in docker but db in host, use: host.docker.internal, ip address. On Docker Desktop for Windows or Mac, the DNS name host.docker.internal will resolve to the host machine's address by default, allowing container programs to access the host through this name. However, on Linux, Docker does not configure this resolution by default, and you should use the host machine's address directly.
# other cases, use the host name: localhost, ip address, or url
VECTOR_STORE_DB_HOST=mem0-qdrant
# The port where the vector db binding to host
VECTOR_STORE_DB_PORT=6333
# API key for the vector db server
VECTOR_STORE_DB_API_KEY=

# Embedding Model
VECTOR_STORE_EMBEDDING_PROVIDER=huggingface
VECTOR_STORE_EMBEDDING_MODEL=multi-qa-MiniLM-L6-cos-v1
# Dimensions of the embedding model
# Diamention of multi-qa-MiniLM-L6-cos-v1 is 384
VECTOR_STORE_EMBEDDING_MODEL_DIMS=384

# Graph Store
GRAPH_STORE_DB_HOST=mem0-neo4j
GRAPH_STORE_DB_PORT=7687
GRAPH_STORE_DB_USERNAME=neo4j
GRAPH_STORE_DB_PASSWORD=mem0
GRAPH_STORE_LLM_PROVIDER=openai
GRAPH_STORE_LLM_MODEL=gpt-4o-mini
GRAPH_STORE_LLM_TEMPERATURE=0.0
# Mem0-Dify Project
# When set, the API will use this AUTH_KEY to authenticate the client
MEM0_API_AUTH_KEY=

# Ensure network name equal to dify api docker's network name
NETWORK=docker_default

# Use to generate web service API schema, fill the mem0-api host and port that dify can reach.
# When mem0-api and dify running in the same docker network, use the docker container name: mem0-api
# When dify in docker but mem0-api in host, use: host.docker.internal, ip address. On Docker Desktop for Windows or Mac, the DNS name host.docker.internal will resolve to the host machine's address by default, allowing container programs to access the host through this name. However, on Linux, Docker does not configure this resolution by default, and you should use the host machine's address directly.
# When other cases, use the mem0-api host name: localhost, ip address, or url
MEM0_API_HOST=mem0-api
# The port where the mem0-api binding to host
MEM0_API_PORT=8000